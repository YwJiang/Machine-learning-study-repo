{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from gensim import corpora\n",
    "from collections import Counter\n",
    "from langdetect import detect_langs\n",
    "import collections\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data, clean data, and transfer to list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/trg.csv', header=0,index_col=0)\n",
    "new_data = pd.read_csv('../data/tst.csv', header=0,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    \n",
    "    # token and split into list of words\n",
    "    df_tokens = []\n",
    "    for i in range(len(df.abstract.values)):\n",
    "        df_tokens.append(nltk.word_tokenize(df.abstract.values[i]))\n",
    "        \n",
    "    # delete stopwords\n",
    "    df_token_stopwords = []\n",
    "    for i in range(len(df.abstract.values)):\n",
    "        df_token_stopwords.append([w for w in df_tokens[i] if w not in stopwords.words('english')])\n",
    "    df['clean'] = df_token_stopwords\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess(data)\n",
    "new_df = preprocess(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split to train, test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df,df['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an accuracy caculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_accuracy(test, real):\n",
    "    return np.mean([test[i] == real[i] for i in range(len(real))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Naive Bayes Implement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes():\n",
    "    \n",
    "    def __init__(self, df, new_df):\n",
    "        self.df = df\n",
    "        self.new_df = new_df\n",
    "        self.instances_num = df.shape[0]\n",
    "        self.J = 0\n",
    "\n",
    "        \n",
    "    def count_attributes(self):\n",
    "        \"\"\"\n",
    "        count all words in df\n",
    "        input: df with clean column\n",
    "        output: cnt dictionary of word list\n",
    "        \"\"\"\n",
    "        cnt = collections.Counter()\n",
    "        for word in self.df['clean'].values:\n",
    "            for i in word:\n",
    "                cnt[i] += 1\n",
    "        self.count_attributes = cnt\n",
    "        \n",
    "\n",
    " \n",
    "    def count_class(self):\n",
    "        \"\"\"\n",
    "        count the number of class labels\n",
    "        input: df with class\n",
    "        output: cnt dictionary of label\n",
    "        \"\"\"\n",
    "        cnt2 = collections.Counter()\n",
    "        for i in self.df['class'].values:\n",
    "            cnt2[i] += 1\n",
    "        self.count_class = dict(cnt2)\n",
    "\n",
    "\n",
    "    def count_prior(self):\n",
    "        \"\"\"\n",
    "        count the frequency of words in each class, return a dictionary(four class) of dictionary(words and freqeuncy)\n",
    "        input: df with class and clean column\n",
    "        output: list with four lists each for a class\n",
    "        \"\"\"\n",
    "        result_list = {}\n",
    "        cnt_A = collections.Counter()\n",
    "        cnt_B = collections.Counter()\n",
    "        cnt_V = collections.Counter()\n",
    "        cnt_E = collections.Counter()\n",
    "        for i in range(len(self.df['class'].values)):\n",
    "            if self.df['class'].values[i] == 'A':\n",
    "                for i in self.df['clean'].values[i]:\n",
    "                    cnt_A[i] += 1\n",
    "            elif self.df['class'].values[i] == 'B':\n",
    "                for i in self.df['clean'].values[i]:\n",
    "                    cnt_B[i] += 1\n",
    "            elif self.df['class'].values[i] == 'E':\n",
    "                for i in self.df['clean'].values[i]:\n",
    "                    cnt_E[i] += 1\n",
    "            elif self.df['class'].values[i] == 'V':\n",
    "                for i in self.df['clean'].values[i]:\n",
    "                    cnt_V[i] += 1\n",
    "        result_list['A'] = cnt_A\n",
    "        result_list['B'] = cnt_B\n",
    "        result_list['E'] = cnt_E\n",
    "        result_list['V'] = cnt_V\n",
    "\n",
    "        self.count_prior =  result_list\n",
    "    \n",
    "   \n",
    "    def count_class_attributes(self):\n",
    "        \"\"\"\n",
    "        count how many words in class[A|B|E|V]\n",
    "        input:\n",
    "        output: list of class and number of list\n",
    "        \"\"\"\n",
    "        class_attributes = {}\n",
    "        for i in ['A','B','V','E']:\n",
    "            class_attributes[i] = sum(self.count_prior[i].values())\n",
    "            self.J += len(self.count_prior[i].keys())\n",
    "        self.count_class_attributes = class_attributes\n",
    "        self.total_num_words = sum(self.count_class_attributes.values())\n",
    "    \n",
    "\n",
    "        \n",
    "    def predict_naive(self, mu):\n",
    "        \"\"\"\n",
    "        predict lables, without im\n",
    "        input: df with classes, clean\n",
    "        output: df with predict label\n",
    "        \"\"\"\n",
    "\n",
    "        prob = {}\n",
    "        result = []\n",
    "        for i in range(len(self.new_df['abstract'])):\n",
    "            a, b, e, v = 0, 0, 0, 0\n",
    "            for j in self.new_df['clean'].values[i]:\n",
    "                a += math.log(self.count_prior['A'][j]+mu)-math.log(self.count_class_attributes['A']+mu*self.J)                                                                                                                                                                                                                                                         # 分子\n",
    "                b += math.log(self.count_prior['B'][j]+mu)-math.log(self.count_class_attributes['B']+mu*self.J) \n",
    "                e += math.log(self.count_prior['E'][j]+mu)-math.log(self.count_class_attributes['E']+mu*self.J) \n",
    "                v += math.log(self.count_prior['V'][j]+mu)-math.log(self.count_class_attributes['V']+mu*self.J) \n",
    "            prob['A'] = (a)+math.log(self.count_class['A']) \n",
    "            prob['B'] = (b)+math.log(self.count_class['B'])\n",
    "            prob['E'] = (e)+math.log(self.count_class['E']) \n",
    "            prob['V'] = (v)+math.log(self.count_class['V']) \n",
    "            result.append(max(prob,key=prob.get)) # get the index of max value in the dic\n",
    "        self.predict_label = result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB accuracy is:  0.96625\n"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayes(X_train, X_test)\n",
    "nb.count_attributes()\n",
    "nb.count_class()\n",
    "nb.count_prior()\n",
    "nb.count_class_attributes()\n",
    "nb.predict_naive(1)\n",
    "print(\"NB accuracy is: \",cal_accuracy(nb.predict_label, y_test.values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complement Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Complement_NaiveBayes():\n",
    "    \n",
    "    def __init__(self, df, new_df):\n",
    "        self.df = df\n",
    "        self.new_df = new_df\n",
    "        self.instances_num = df.shape[0]\n",
    "        self.J = 0\n",
    "\n",
    "        \n",
    "    def count_attributes(self):\n",
    "        \"\"\"\n",
    "        count all words in df\n",
    "        input: df with clean column\n",
    "        output: cnt dictionary of word list\n",
    "        \"\"\"\n",
    "        cnt = collections.Counter()\n",
    "        for word in self.df['clean'].values:\n",
    "            for i in word:\n",
    "                cnt[i] += 1\n",
    "        self.count_attributes = cnt\n",
    "        \n",
    "\n",
    " \n",
    "    def count_class(self):\n",
    "        \"\"\"\n",
    "        count the number of class labels\n",
    "        input: df with class\n",
    "        output: cnt dictionary of label\n",
    "        \"\"\"\n",
    "        cnt2 = collections.Counter()\n",
    "        for i in self.df['class'].values:\n",
    "            cnt2[i] += 1\n",
    "        self.count_class = dict(cnt2)\n",
    "\n",
    "\n",
    "    def count_prior(self):\n",
    "        \"\"\"\n",
    "        count the frequency of words in each class, return a dictionary(four class) of dictionary(words and freqeuncy)\n",
    "        input: df with class and clean column\n",
    "        output: list with four lists each for a class\n",
    "        \"\"\"\n",
    "        result_list = {}\n",
    "        cnt_A = collections.Counter()\n",
    "        cnt_B = collections.Counter()\n",
    "        cnt_V = collections.Counter()\n",
    "        cnt_E = collections.Counter()\n",
    "        for i in range(len(self.df['class'].values)):\n",
    "            if self.df['class'].values[i] == 'A':\n",
    "                for i in self.df['clean'].values[i]:\n",
    "                    cnt_A[i] += 1\n",
    "            elif self.df['class'].values[i] == 'B':\n",
    "                for i in self.df['clean'].values[i]:\n",
    "                    cnt_B[i] += 1\n",
    "            elif self.df['class'].values[i] == 'E':\n",
    "                for i in self.df['clean'].values[i]:\n",
    "                    cnt_E[i] += 1\n",
    "            elif self.df['class'].values[i] == 'V':\n",
    "                for i in self.df['clean'].values[i]:\n",
    "                    cnt_V[i] += 1\n",
    "        result_list['A'] = cnt_A\n",
    "        result_list['B'] = cnt_B\n",
    "        result_list['E'] = cnt_E\n",
    "        result_list['V'] = cnt_V\n",
    "        self.count_prior =  result_list\n",
    "    \n",
    "   \n",
    "    def count_class_attributes(self):\n",
    "        \"\"\"\n",
    "        count how many words in class[A|B|E|V]\n",
    "        input:\n",
    "        output: list of class and number of list\n",
    "        \"\"\"\n",
    "        class_attributes = {}\n",
    "        for i in ['A','B','V','E']:\n",
    "            class_attributes[i] = sum(self.count_prior[i].values())\n",
    "            self.J += len(self.count_prior[i].keys())\n",
    "        self.count_class_attributes = class_attributes\n",
    "        self.total_num_words = sum(self.count_class_attributes.values())\n",
    "\n",
    "        \n",
    "        \n",
    "    def predict_complement_naive(self,mu):\n",
    "        \"\"\"\n",
    "        predict the posterior probability based on complement naive bayes\n",
    "        output: a list of predicting labels\n",
    "        \"\"\"\n",
    "        prob2 = {}\n",
    "        result2 = []\n",
    "        for i in range(len(self.new_df['abstract'])):\n",
    "            a2, b2, e2, v2 = 0, 0, 0, 0\n",
    "            for j in self.new_df['clean'].values[i]:\n",
    "                a2 -= math.log(self.count_attributes[j]-self.count_prior['A'][j]+mu) - math.log(self.total_num_words-self.count_class_attributes['A']+mu*self.J)                                                                                                                                                                                                                                                         # 分子\n",
    "                b2 -= math.log(self.count_attributes[j]-self.count_prior['B'][j]+mu) - math.log(self.total_num_words-self.count_class_attributes['B']+mu*self.J) # 分子\n",
    "                e2 -= math.log(self.count_attributes[j]-self.count_prior['E'][j]+mu) - math.log(self.total_num_words-self.count_class_attributes['E']+mu*self.J) # 分子\n",
    "                v2 -= math.log(self.count_attributes[j]-self.count_prior['V'][j]+mu) - math.log(self.total_num_words-self.count_class_attributes['V']+mu*self.J) # 分子\n",
    "            prob2['A'] = (a2)+math.log(self.count_class['A']) \n",
    "            prob2['B'] = (b2)+math.log(self.count_class['B']) \n",
    "            prob2['E'] = (e2)+math.log(self.count_class['E']) \n",
    "            prob2['V'] = (v2)+math.log(self.count_class['V']) \n",
    "            result2.append(max(prob2,key=prob2.get))\n",
    "        self.predict_label_cnb = result2\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB accuracy is:  0.9725\n"
     ]
    }
   ],
   "source": [
    "cnb = Complement_NaiveBayes(X_train, X_test)\n",
    "cnb.count_attributes()\n",
    "cnb.count_class()\n",
    "cnb.count_prior()\n",
    "cnb.count_class_attributes()\n",
    "cnb.predict_complement_naive(1)\n",
    "print(\"NB accuracy is: \",cal_accuracy(cnb.predict_label_cnb, y_test.values))\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer count data by document frequency\n",
    "Most of the structure is same as the naive Bayes. What I have done here is just transfer the count of word, which is based on the document frequency to reduce the noise data, from 1 to  ![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transfer_Complement_NaiveBayes():\n",
    "    \n",
    "    def __init__(self, df, new_df):\n",
    "        self.df = df\n",
    "        self.new_df = new_df\n",
    "        self.instances_num = df.shape[0]\n",
    "        self.J = 0\n",
    "        \n",
    "    def transfer_docfre_count_(self):\n",
    "        \"\"\"\n",
    "        count all words in df\n",
    "        input: df with clean column\n",
    "        output: cnt dictionary of word list\n",
    "        \"\"\"\n",
    "        cnt = collections.Counter()\n",
    "        for word in self.df['clean'].values:\n",
    "            for i in word:\n",
    "                cnt[i] += 1\n",
    "        \n",
    "        cnt3 = collections.Counter()\n",
    "        for words in self.df['clean'].values:\n",
    "            for i in cnt.keys():\n",
    "                if i in words:\n",
    "                    cnt3[i]+=1\n",
    "        self.count_attributes_docfre = cnt3\n",
    "        \n",
    "        cnt4 = collections.Counter()\n",
    "        for word in self.df['clean'].values:\n",
    "            for i in word:\n",
    "                cnt4[i] += 1*math.log(self.instances_num/self.count_attributes_docfre[i]) # instead of add 1 every time meet a word, add another weight\n",
    "        self.count_attributes = cnt4\n",
    "        \n",
    "\n",
    " \n",
    "    def count_class(self):\n",
    "        \"\"\"\n",
    "        count the number of class labels\n",
    "        input: df with class\n",
    "        output: cnt dictionary of label\n",
    "        \"\"\"\n",
    "        cnt2 = collections.Counter()\n",
    "        for i in self.df['class'].values:\n",
    "            cnt2[i] += 1\n",
    "        self.count_class = dict(cnt2)\n",
    "\n",
    "\n",
    "    def count_prior(self):\n",
    "        \"\"\"\n",
    "        count the frequency of words in each class, return a dictionary(four class) of dictionary(words and freqeuncy)\n",
    "        input: df with class and clean column\n",
    "        output: list with four lists each for a class\n",
    "        \"\"\"\n",
    "        result_list = {}\n",
    "        cnt_A = collections.Counter()\n",
    "        cnt_B = collections.Counter()\n",
    "        cnt_V = collections.Counter()\n",
    "        cnt_E = collections.Counter()\n",
    "        for i in range(len(self.df['class'].values)):\n",
    "            if self.df['class'].values[i] == 'A':\n",
    "                for i in self.df['clean'].values[i]:\n",
    "                    cnt_A[i] += 1*math.log(self.instances_num/self.count_attributes_docfre[i])\n",
    "            elif self.df['class'].values[i] == 'B':\n",
    "                for i in self.df['clean'].values[i]:\n",
    "                    cnt_B[i] += 1*math.log(self.instances_num/self.count_attributes_docfre[i])\n",
    "            elif self.df['class'].values[i] == 'E':\n",
    "                for i in self.df['clean'].values[i]:\n",
    "                    cnt_E[i] += 1*math.log(self.instances_num/self.count_attributes_docfre[i])\n",
    "            elif self.df['class'].values[i] == 'V':\n",
    "                for i in self.df['clean'].values[i]:\n",
    "                    cnt_V[i] += 1*math.log(self.instances_num/self.count_attributes_docfre[i])\n",
    "        result_list['A'] = cnt_A\n",
    "        result_list['B'] = cnt_B\n",
    "        result_list['E'] = cnt_E\n",
    "        result_list['V'] = cnt_V\n",
    "\n",
    "        self.count_prior =  result_list\n",
    "    \n",
    "   \n",
    "    def count_class_attributes(self):\n",
    "        \"\"\"\n",
    "        count how many words in class[A|B|E|V]\n",
    "        input:\n",
    "        output: list of class and number of list\n",
    "        \"\"\"\n",
    "        class_attributes = {}\n",
    "        for i in ['A','B','V','E']:\n",
    "            class_attributes[i] = sum(self.count_prior[i].values())\n",
    "            self.J += len(self.count_prior[i].keys())\n",
    "        self.count_class_attributes = class_attributes\n",
    "        self.total_num_words = sum(self.count_class_attributes.values())\n",
    "    \n",
    "\n",
    "        \n",
    "    def predict_complement_naive(self,mu):\n",
    "        \"\"\"\n",
    "        predict the posterior probability based on complement naive bayes\n",
    "        output: a list of predicting labels\n",
    "        \"\"\"\n",
    "        prob2 = {}\n",
    "        result2 = []\n",
    "        for i in range(len(self.new_df['abstract'])):\n",
    "            a2, b2, e2, v2 = 0, 0, 0, 0\n",
    "            for j in self.new_df['clean'].values[i]:\n",
    "                a2 -= math.log(self.count_attributes[j]-self.count_prior['A'][j]+mu) - math.log(self.total_num_words-self.count_class_attributes['A']+mu*self.J)                                                                                                                                                                                                                                                         # 分子\n",
    "                b2 -= math.log(self.count_attributes[j]-self.count_prior['B'][j]+mu) - math.log(self.total_num_words-self.count_class_attributes['B']+mu*self.J) \n",
    "                e2 -= math.log(self.count_attributes[j]-self.count_prior['E'][j]+mu) - math.log(self.total_num_words-self.count_class_attributes['E']+mu*self.J) \n",
    "                v2 -= math.log(self.count_attributes[j]-self.count_prior['V'][j]+mu) - math.log(self.total_num_words-self.count_class_attributes['V']+mu*self.J) \n",
    "            prob2['A'] = (a2)+math.log(self.count_class['A']) \n",
    "            prob2['B'] = (b2)+math.log(self.count_class['B']) \n",
    "            prob2['E'] = (e2)+math.log(self.count_class['E']) \n",
    "            prob2['V'] = (v2)+math.log(self.count_class['V']) \n",
    "            result2.append(max(prob2,key=prob2.get))\n",
    "        self.predict_label_cnb = result2\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCNB accuracy is:  0.9825\n"
     ]
    }
   ],
   "source": [
    "tcnb = Transfer_Complement_NaiveBayes(X_train, X_test)\n",
    "tcnb.transfer_docfre_count_()\n",
    "\n",
    "tcnb.count_class()\n",
    "tcnb.count_prior()\n",
    "tcnb.count_class_attributes()\n",
    "tcnb.predict_complement_naive(1)\n",
    "print(\"TCNB accuracy is: \",cal_accuracy(tcnb.predict_label_cnb, y_test.values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform by the length of the doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transfer_2nd_Complement_NaiveBayes():\n",
    "    \n",
    "    def __init__(self, df, new_df):\n",
    "        self.df = df\n",
    "        self.new_df = new_df\n",
    "        self.instances_num = df.shape[0]\n",
    "        self.J = 0\n",
    "        \n",
    "    def transfer_2nd_docfre_count_(self):\n",
    "        \"\"\"\n",
    "        count all words in df\n",
    "        input: df with clean column\n",
    "        output: cnt dictionary of word list\n",
    "        \"\"\"\n",
    "        cnt = collections.Counter()\n",
    "        for word in self.df['clean'].values:\n",
    "            for i in word:\n",
    "                cnt[i] += 1\n",
    "                \n",
    "                \n",
    "        cnt3 = collections.Counter()\n",
    "        for words in self.df['clean'].values:\n",
    "            for i in cnt.keys():\n",
    "                if i in words:\n",
    "                    cnt3[i]+=1\n",
    "        self.count_attributes_docfre = cnt3\n",
    "        \n",
    "        cnt5 = collections.Counter()\n",
    "        cnt6 = collections.Counter()\n",
    "        for words in self.df['clean'].values:\n",
    "            for i in words:\n",
    "                cnt5[i]+=1\n",
    "            print(cnt5,type(cnt5),list(cnt5.values()),type(list(cnt5.values())))\n",
    "            cnt6[words] = math.sqrt(sum((np.array(list(cnt5.values())))**2))\n",
    "            cnt5 = collections.Counter()\n",
    "        self.cnt5 = cnt5\n",
    "        self.count_attributes_doclen = cnt6\n",
    "        \n",
    "        cnt4 = collections.Counter()\n",
    "        for word in self.df['clean'].values:\n",
    "            for i in word:\n",
    "                cnt4[i] += 1*math.log(self.instances_num/self.count_attributes_docfre[i])/count_attributes_doclen[words] # instead of add 1 every time meet a word, add another weight\n",
    "        self.count_attributes = cnt4\n",
    "        \n",
    "\n",
    " \n",
    "    def count_class(self):\n",
    "        \"\"\"\n",
    "        count the number of class labels\n",
    "        input: df with class\n",
    "        output: cnt dictionary of label\n",
    "        \"\"\"\n",
    "        cnt2 = collections.Counter()\n",
    "        for i in self.df['class'].values:\n",
    "            cnt2[i] += 1\n",
    "        self.count_class = dict(cnt2)\n",
    "\n",
    "\n",
    "    def count_prior(self):\n",
    "        \"\"\"\n",
    "        count the frequency of words in each class, return a dictionary(four class) of dictionary(words and freqeuncy)\n",
    "        input: df with class and clean column\n",
    "        output: list with four lists each for a class\n",
    "        \"\"\"\n",
    "        result_list = {}\n",
    "        cnt_A = collections.Counter()\n",
    "        cnt_B = collections.Counter()\n",
    "        cnt_V = collections.Counter()\n",
    "        cnt_E = collections.Counter()\n",
    "        for i in range(len(self.df['class'].values)):\n",
    "            if self.df['class'].values[i] == 'A':\n",
    "                for i in self.df['clean'].values[i]:\n",
    "                    cnt_A[i] += 1*math.log(self.instances_num/self.count_attributes_docfre[i])\n",
    "            elif self.df['class'].values[i] == 'B':\n",
    "                for i in self.df['clean'].values[i]:\n",
    "                    cnt_B[i] += 1*math.log(self.instances_num/self.count_attributes_docfre[i])\n",
    "            elif self.df['class'].values[i] == 'E':\n",
    "                for i in self.df['clean'].values[i]:\n",
    "                    cnt_E[i] += 1*math.log(self.instances_num/self.count_attributes_docfre[i])\n",
    "            elif self.df['class'].values[i] == 'V':\n",
    "                for i in self.df['clean'].values[i]:\n",
    "                    cnt_V[i] += 1*math.log(self.instances_num/self.count_attributes_docfre[i])\n",
    "        result_list['A'] = cnt_A\n",
    "        result_list['B'] = cnt_B\n",
    "        result_list['E'] = cnt_E\n",
    "        result_list['V'] = cnt_V\n",
    "\n",
    "        self.count_prior =  result_list\n",
    "    \n",
    "   \n",
    "    def count_class_attributes(self):\n",
    "        \"\"\"\n",
    "        count how many words in class[A|B|E|V]\n",
    "        input:\n",
    "        output: list of class and number of list\n",
    "        \"\"\"\n",
    "        class_attributes = {}\n",
    "        for i in ['A','B','V','E']:\n",
    "            class_attributes[i] = sum(self.count_prior[i].values())\n",
    "            self.J += len(self.count_prior[i].keys())\n",
    "        self.count_class_attributes = class_attributes\n",
    "        self.total_num_words = sum(self.count_class_attributes.values())\n",
    "    \n",
    "\n",
    "        \n",
    "    def predict_complement_naive(self,mu):\n",
    "        \"\"\"\n",
    "        predict the posterior probability based on complement naive bayes\n",
    "        output: a list of predicting labels\n",
    "        \"\"\"\n",
    "        prob2 = {}\n",
    "        result2 = []\n",
    "        for i in range(len(self.new_df['abstract'])):\n",
    "            a2, b2, e2, v2 = 0, 0, 0, 0\n",
    "            for j in self.new_df['clean'].values[i]:\n",
    "                a2 -= math.log(self.count_attributes[j]-self.count_prior['A'][j]+mu) - math.log(self.total_num_words-self.count_class_attributes['A']+mu*self.J)                                                                                                                                                                                                                                                         # 分子\n",
    "                b2 -= math.log(self.count_attributes[j]-self.count_prior['B'][j]+mu) - math.log(self.total_num_words-self.count_class_attributes['B']+mu*self.J) \n",
    "                e2 -= math.log(self.count_attributes[j]-self.count_prior['E'][j]+mu) - math.log(self.total_num_words-self.count_class_attributes['E']+mu*self.J) \n",
    "                v2 -= math.log(self.count_attributes[j]-self.count_prior['V'][j]+mu) - math.log(self.total_num_words-self.count_class_attributes['V']+mu*self.J) \n",
    "            prob2['A'] = (a2)+math.log(self.count_class['A']) \n",
    "            prob2['B'] = (b2)+math.log(self.count_class['B']) \n",
    "            prob2['E'] = (e2)+math.log(self.count_class['E']) \n",
    "            prob2['V'] = (v2)+math.log(self.count_class['V']) \n",
    "            result2.append(max(prob2,key=prob2.get))\n",
    "        self.predict_label_cnb = result2\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'genome': 5, 'c-125': 4, 'halodurans': 3, 'cdss': 3, 'bacillus': 2, 'contains': 2, 'protein': 2, 'sequences': 2, 'conserved': 2, 'function': 2, 'match': 2, 'proteins': 2, 'found': 2, 'bsubtilis': 2, 'b': 2, 'genes': 2, 'role': 2, 'competence': 2, 'tupa': 2, '4': 1, '202': 1, '353': 1, 'bp': 1, 'alkaliphilic': 1, 'bacterium': 1, '4066': 1, 'predicted': 1, 'coding': 1, '2141': 1, '527': 1, 'functional': 1, 'assignments': 1, '1182': 1, '29': 1, 'unknown': 1, '743': 1, '18': 1, '3': 1, 'database': 1, 'among': 1, 'total': 1, '88': 1, 'subtilis': 1, '667': 1, 'widely': 1, 'comparison': 1, 'various': 1, 'organisms': 1, 'including': 1, '112': 1, 'transposase': 1, 'indicating': 1, 'transposases': 1, 'played': 1, 'important': 1, 'evolutionary': 1, 'horizontal': 1, 'gene': 1, 'transfer': 1, 'also': 1, 'internal': 1, 'genetic': 1, 'rearrangement': 1, 'strain': 1, 'lacks': 1, 'necessary': 1, 'coms': 1, 'srfa': 1, 'rapc': 1, 'supporting': 1, 'fact': 1, 'demonstrated': 1, 'experimentally': 1, 'paralog': 1, 'encoding': 1, 'teichuronopeptide': 1, 'contributes': 1, 'alkaliphily': 1, 'ortholog': 1, '11': 1, 'sigma': 1, 'factors': 1, 'belong': 1, 'extracytoplasmic': 1, 'family': 1, '10': 1, 'unique': 1, 'suggesting': 1, 'may': 1, 'special': 1, 'mechanism': 1, 'adaptation': 1, 'alkaline': 1, 'environment': 1}) <class 'collections.Counter'> [1, 1, 1, 1, 5, 1, 1, 2, 3, 4, 2, 1, 1, 2, 1, 2, 3, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] <class 'list'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-ac13e4848716>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTransfer_2nd_Complement_NaiveBayes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransfer_2nd_docfre_count_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m '''\n\u001b[0;32m      4\u001b[0m \u001b[0mnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_prior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-c19f664617b3>\u001b[0m in \u001b[0;36mtransfer_2nd_docfre_count_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[0mcnt5\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnt5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnt5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnt5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnt5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0mcnt6\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnt5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[0mcnt5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcnt5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnt5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "nb = Transfer_2nd_Complement_NaiveBayes(X, X)\n",
    "nb.transfer_2nd_docfre_count_()\n",
    "'''\n",
    "nb.count_class()\n",
    "nb.count_prior()\n",
    "nb.count_class_attributes()\n",
    "nb.predict_naive(1)\n",
    "print(\"NB accuracy is: \",cal_accuracy(nb.predict_label, y_test.values))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ten cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ten_CV_nb(df, split_size= 0.3, num_times = 10, num_folds=10):\n",
    "    X, y = df,df['class']\n",
    "    nb_res_list = []\n",
    "    random_seeds = [int(x) for x in np.random.uniform(0, 10000, num_times)]\n",
    "    for i in random_seeds:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_size, random_state=i)\n",
    "        nb = NaiveBayes(X_train, X_test)\n",
    "        nb.count_attributes()\n",
    "        nb.count_class()\n",
    "        nb.count_prior()\n",
    "        nb.count_class_attributes()\n",
    "        nb.predict_naive(1)\n",
    "        nb_res_list.append(cal_accuracy(nb.predict_label, y_test.values))\n",
    "    return np.mean(nb_res_list), np.var(nb_res_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Simple NB is 0.9443333333333334 and the variance is 4.427777777777777e-05\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy of Simple NB is %s and the variance is %s'%ten_CV_nb(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ten_CV_cnb(df, split_size= 0.3, num_times = 10, num_folds=10):\n",
    "    X, y = df,df['class']\n",
    "    cnb_res_list = []\n",
    "    random_seeds = [int(x) for x in np.random.uniform(0, 10000, num_times)]\n",
    "    for i in random_seeds:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_size, random_state=i)\n",
    "        cnb = Complement_NaiveBayes(X_train, X_test)\n",
    "        cnb.count_attributes()\n",
    "        cnb.count_class()\n",
    "        cnb.count_prior()\n",
    "        cnb.count_class_attributes()\n",
    "        cnb.predict_complement_naive(1)\n",
    "        cnb_res_list.append(cal_accuracy(cnb.predict_label_cnb, y_test.values))\n",
    "    return np.mean(cnb_res_list), np.var(cnb_res_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Complement NB is 0.9605 and the variance is 4.8916666666666726e-05\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy of Complement NB is %s and the variance is %s'%ten_CV_cnb(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TCNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ten_CV_tcnb(df, split_size= 0.3, num_times = 10, num_folds=10):\n",
    "    X, y = df,df['class']\n",
    "    tcnb_res_list = []\n",
    "    random_seeds = [int(x) for x in np.random.uniform(0, 10000, num_times)]\n",
    "    for i in random_seeds:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_size, random_state=i)\n",
    "        tcnb = Transfer_Complement_NaiveBayes(X_train, X_test)\n",
    "        tcnb.transfer_docfre_count_()\n",
    "\n",
    "        tcnb.count_class()\n",
    "        tcnb.count_prior()\n",
    "        tcnb.count_class_attributes()\n",
    "        tcnb.predict_complement_naive(1)\n",
    "        tcnb_res_list.append(cal_accuracy(tcnb.predict_label_cnb, y_test.values))\n",
    "    return np.mean(tcnb_res_list), np.var(tcnb_res_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Transfer Complement NB is 0.9658333333333333 and the variance is 1.444444444444439e-05\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy of Transfer Complement NB is %s and the variance is %s'%ten_CV_tcnb(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the result to .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "tcnb = Transfer_Complement_NaiveBayes(df, new_df)\n",
    "tcnb.transfer_docfre_count_()\n",
    "tcnb.count_class()\n",
    "tcnb.count_prior()\n",
    "tcnb.count_class_attributes()\n",
    "tcnb.predict_complement_naive(1)\n",
    "new_df['class'] = tcnb.predict_label_cnb\n",
    "result_df_simple = copy.deepcopy(new_df)\n",
    "#result_df_simple.drop([\"abstract\"], axis = 1).drop(['clean'], axis = 1).to_csv(\"../data/tst_kaggle3_transfer_complement_tran.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4618006"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.array(list(cnb.count_class.values()))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list(cnb.count_class.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13321.003753471432"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt(sum(np.array((list(dict(cnb.count_attributes).values())))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
